{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is not\" with 'int' literal. Did you mean \"!=\"?\n",
      "<>:12: SyntaxWarning: \"is not\" with 'int' literal. Did you mean \"!=\"?\n",
      "C:\\Users\\suzuk\\AppData\\Local\\Temp\\ipykernel_17356\\1181907894.py:12: SyntaxWarning: \"is not\" with 'int' literal. Did you mean \"!=\"?\n",
      "  except round(d_model/n_heads,0) % n_heads is not 0:\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    #input_dim = 임베딩 차원 / d_model = q,k,v가 가질 차원의 크기\n",
    "  def __init__(self, input_dim, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    try:\n",
    "        self.input_dim = input_dim\n",
    "        self.d_prime_model = int(round(d_model/n_heads,0))\n",
    "        self.n_heads = n_heads\n",
    "    except round(d_model/n_heads,0) % n_heads is not 0:\n",
    "        print('round(d_model/n_heads,0) % n_heads is not 0')\n",
    "        \n",
    "    self.wq = nn.Linear(input_dim, d_model) #쿼리벡터 생성\n",
    "    self.wk = nn.Linear(input_dim, d_model) #키 벡터 생성\n",
    "    self.wv = nn.Linear(input_dim, d_model) #밸류 벡터 생성\n",
    "    self.dense = nn.Linear(d_model, d_model) #어텐션 최종 out 을 위해.\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "\n",
    "  def forward(self, x, mask):   \n",
    "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "    # B S H D' 에서 B H S D'으로 변경 필요\n",
    "    # B S H D' 은 batch 안에 sequentioal가 key인 H vs D' 행렬이 존재하므로 Head가 종속적\n",
    "    # B H S D'은 batch 안에 head가 key 인 S vs D' 행렬이 존재 하므로 Head가 독립적\n",
    "    q = q.view(q.shape[0], q.shape[1], self.n_heads, self.d_prime_model).transpose(1,2)\n",
    "    k = k.view(k.shape[0], k.shape[1], self.n_heads, self.d_prime_model).transpose(1,2)\n",
    "    v = v.view(v.shape[0], v.shape[1], self.n_heads, self.d_prime_model).transpose(1,2)\n",
    "\n",
    "    # B H S D' * B H D' S = B H S S\n",
    "    # 각 head에 따른 상관도를 확인 가능\n",
    "    score = torch.matmul(q, k.transpose(-1, -2)) \n",
    "    score = score / sqrt(self.d_prime_model)\n",
    "    \n",
    "    if mask is not None:\n",
    "    #마스크가 있다면 실행, 마스크가 있는곳은 1로 되어져있으므로 가장 작은수를 곱하고 더함\n",
    "    #soft max 함수 이후에 0에 근사함\n",
    "    #mask(B S S) 와 현 score(B H S S) 텐서 형식이 다르므로 통일 해줘야함.\n",
    "        mask = mask.unsqueeze(1)\n",
    "    #unsqueeze를 통해 B 1 S S로 만듬\n",
    "        score = score + (mask * -1e9)\n",
    "\n",
    "    score = self.softmax(score) # B H S S\n",
    "    result = torch.matmul(score, v) # B H S D'\n",
    "    result = result.transpose(1,2).contiguous()\n",
    "    result = result.view(result.shape[0], result.shape[1], -1) # B S D\n",
    "    result = self.dense(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, dff, dropout):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.d_model = d_model\n",
    "    self.dff = dff\n",
    "\n",
    "    self.MHA = MultiHeadAttention(input_dim, d_model, 4) #Head를 4로 설정\n",
    "\n",
    "    self.ffn = nn.Sequential(\n",
    "      nn.Linear(d_model, dff),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dff, d_model)\n",
    "    )\n",
    "    self.dropout1 = nn.Dropout(p=dropout)  # Overfitting 방지를 위한 Dropout 설정\n",
    "    self.dropout2 = nn.Dropout(p=dropout)  \n",
    "\n",
    "    self.layer_norm1 = nn.LayerNorm(d_model)  # Layer Normalization 정의\n",
    "    self.layer_norm2 = nn.LayerNorm(d_model)  \n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    x1 = self.MHA(x, mask) #Multi Head Attention 진행\n",
    "    x1 = self.dropout1(x1) #Dropout\n",
    "    x1 = self.layer_norm1(x1 + x) #잔차연결 후 층 정규화\n",
    "\n",
    "    x2 = self.ffn(x1) #Multi Head Attention + 층정규화 진행된 값을 Feed Forward\n",
    "    x2 = self.dropout2(x2)\n",
    "    x2 = self.layer_norm2(x2 + x1) #잔차연결 후 층 정규화\n",
    "\n",
    "    return x2 #최종 Encoder 출력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, dff, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.MHA = MultiHeadAttention(input_dim, d_model, n_heads=4)\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Layer Normalization layers\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Self-attention\n",
    "        x1 = self.MHA(x, mask)  # Multi-Head Attention 적용\n",
    "        x1 = self.dropout1(x1)  # Dropout\n",
    "        x1 = self.layer_norm1(x1 + x)  # Residual connection + Layer Normalization\n",
    "\n",
    "        # Feed-forward\n",
    "        x2 = self.ffn(x1)  # Feed-forward 네트워크\n",
    "        x2 = self.dropout2(x2)  # Dropout\n",
    "        x2 = self.layer_norm2(x2 + x1)  # Residual connection + Layer Normalization\n",
    "\n",
    "        return x2  # 최종 Decoder 출력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, d_model, dff, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(1000, d_model)  # 최대 길이 1000\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(input_dim, d_model, dff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Positional Encoding 추가\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        x = self.embedding(x) + self.positional_encoding(positions)\n",
    "\n",
    "        # Encoder layers 통과\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x  # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, d_model, dff, vocab_size, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.positional_encoding = nn.Embedding(1000, d_model)  # 최대 길이 1000\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(input_dim, d_model, dff, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        # Linear layer for vocab size output\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        # Positional Encoding 추가\n",
    "        positions = torch.arange(0, tgt.size(1), device=tgt.device).unsqueeze(0)\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding(positions)\n",
    "\n",
    "        # Decoder layers 통과\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, tgt_mask)  # Target Masking\n",
    "\n",
    "        # Linear layer로 vocab_size로 변환\n",
    "        logits = self.linear(tgt)\n",
    "\n",
    "        # Softmax로 확률 변환\n",
    "        probs = self.softmax(logits)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 10000])\n",
      "tensor([[[1.2259e-04, 1.9482e-04, 1.0767e-04,  ..., 1.7829e-04,\n",
      "          1.3503e-04, 7.0349e-05],\n",
      "         [6.9761e-05, 1.8570e-04, 5.8900e-05,  ..., 1.1058e-04,\n",
      "          4.9348e-05, 5.6825e-05],\n",
      "         [6.0946e-05, 1.1298e-04, 9.6323e-05,  ..., 7.2866e-05,\n",
      "          7.6753e-05, 8.9623e-05],\n",
      "         ...,\n",
      "         [1.2898e-04, 2.4305e-04, 6.3202e-05,  ..., 1.8230e-04,\n",
      "          4.3429e-05, 7.0600e-05],\n",
      "         [2.4772e-05, 2.2185e-04, 1.4624e-04,  ..., 6.2857e-05,\n",
      "          4.3389e-05, 1.2707e-04],\n",
      "         [4.5139e-05, 5.7067e-05, 3.6142e-05,  ..., 6.3688e-05,\n",
      "          9.1383e-05, 3.9680e-05]],\n",
      "\n",
      "        [[1.3846e-04, 8.4230e-05, 3.9005e-05,  ..., 8.8730e-05,\n",
      "          1.2755e-04, 3.7343e-05],\n",
      "         [2.9293e-05, 9.9224e-05, 9.8912e-05,  ..., 8.0741e-05,\n",
      "          1.0988e-04, 7.2386e-05],\n",
      "         [1.7453e-05, 1.1815e-04, 2.4781e-04,  ..., 4.4950e-05,\n",
      "          6.0875e-05, 1.2215e-04],\n",
      "         ...,\n",
      "         [1.0958e-04, 1.0414e-04, 7.3630e-05,  ..., 6.9277e-05,\n",
      "          5.3308e-05, 4.2026e-05],\n",
      "         [1.7553e-05, 1.3701e-04, 9.9365e-05,  ..., 4.9086e-05,\n",
      "          4.3611e-05, 7.9454e-05],\n",
      "         [4.0384e-05, 2.0586e-04, 5.3506e-05,  ..., 2.3500e-05,\n",
      "          9.7231e-05, 5.8696e-05]],\n",
      "\n",
      "        [[6.9156e-05, 1.3204e-04, 1.0579e-04,  ..., 2.1595e-04,\n",
      "          9.0560e-05, 8.2689e-05],\n",
      "         [5.5531e-05, 1.2731e-04, 9.7887e-05,  ..., 1.7202e-04,\n",
      "          4.5563e-05, 5.1054e-05],\n",
      "         [2.8031e-05, 1.3432e-04, 5.3901e-05,  ..., 7.6184e-05,\n",
      "          2.6655e-05, 1.3760e-04],\n",
      "         ...,\n",
      "         [4.3966e-05, 9.9982e-05, 1.2181e-04,  ..., 1.0235e-04,\n",
      "          1.0941e-04, 1.9016e-05],\n",
      "         [2.9208e-05, 1.4183e-04, 1.6242e-04,  ..., 6.1788e-05,\n",
      "          1.1412e-04, 5.8851e-05],\n",
      "         [2.9977e-05, 1.4895e-04, 6.5212e-05,  ..., 5.8013e-05,\n",
      "          6.7069e-05, 3.6266e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.1863e-05, 4.6977e-05, 5.3529e-05,  ..., 1.0059e-04,\n",
      "          1.3358e-04, 4.2208e-05],\n",
      "         [3.9570e-05, 8.1367e-05, 1.7190e-04,  ..., 1.3717e-04,\n",
      "          7.9044e-05, 5.4325e-05],\n",
      "         [7.0306e-05, 6.8085e-05, 9.0989e-05,  ..., 1.6277e-04,\n",
      "          1.2448e-04, 1.8415e-04],\n",
      "         ...,\n",
      "         [1.2864e-04, 1.8807e-04, 1.0783e-04,  ..., 1.3257e-04,\n",
      "          8.5709e-05, 2.4833e-05],\n",
      "         [3.1673e-05, 5.5472e-05, 2.1944e-04,  ..., 1.0999e-04,\n",
      "          6.9191e-05, 1.3003e-04],\n",
      "         [3.8793e-05, 4.6105e-05, 6.4209e-05,  ..., 1.2087e-04,\n",
      "          1.2367e-04, 3.6585e-05]],\n",
      "\n",
      "        [[6.4615e-05, 8.7762e-05, 3.7343e-05,  ..., 1.1483e-04,\n",
      "          1.6257e-04, 4.7891e-05],\n",
      "         [5.1404e-05, 1.2132e-04, 1.5365e-04,  ..., 2.3138e-04,\n",
      "          5.8665e-05, 4.6393e-05],\n",
      "         [4.4167e-05, 1.0332e-04, 1.0260e-04,  ..., 1.2869e-04,\n",
      "          3.5503e-05, 1.2310e-04],\n",
      "         ...,\n",
      "         [6.7399e-05, 1.1152e-04, 5.0058e-05,  ..., 9.6499e-05,\n",
      "          1.2252e-04, 3.0089e-05],\n",
      "         [3.2684e-05, 1.4860e-04, 1.8606e-04,  ..., 4.5659e-05,\n",
      "          6.8785e-05, 5.2928e-05],\n",
      "         [3.1197e-05, 1.5310e-04, 1.4191e-04,  ..., 1.4211e-04,\n",
      "          5.9012e-05, 3.3928e-05]],\n",
      "\n",
      "        [[5.4692e-05, 1.5593e-04, 8.7282e-05,  ..., 1.0033e-04,\n",
      "          9.5596e-05, 3.3775e-05],\n",
      "         [3.6762e-05, 2.6962e-04, 1.8062e-04,  ..., 2.6375e-04,\n",
      "          6.7051e-05, 4.6495e-05],\n",
      "         [2.7151e-05, 2.7696e-04, 7.3372e-05,  ..., 1.1771e-04,\n",
      "          3.7430e-05, 4.8779e-05],\n",
      "         ...,\n",
      "         [6.5847e-05, 1.8108e-04, 1.0682e-04,  ..., 1.0751e-04,\n",
      "          4.7899e-05, 2.9654e-05],\n",
      "         [2.0100e-05, 7.7409e-05, 1.5855e-04,  ..., 7.6237e-05,\n",
      "          5.2082e-05, 8.5942e-05],\n",
      "         [5.8566e-05, 1.1307e-04, 1.1028e-04,  ..., 8.7653e-05,\n",
      "          4.2498e-05, 6.4750e-05]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, d_model, dff, vocab_size, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, input_dim, d_model, dff, dropout)\n",
    "        self.decoder = Decoder(num_layers, input_dim, d_model, dff, vocab_size, dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        memory = self.encoder(src, src_mask)  # Encoder에서 출력된 메모리\n",
    "        output = self.decoder(tgt, memory, tgt_mask)  # Decoder에서 예측된 확률 출력\n",
    "        return output\n",
    "\n",
    "\n",
    "# 모델 테스트 예시\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "input_dim = 512  # 입력 차원\n",
    "d_model = 512  # 모델 차원\n",
    "dff = 2048  # Feed-forward 차원\n",
    "vocab_size = 10000  # 단어 집합 크기\n",
    "dropout = 0.1  # Dropout 비율\n",
    "\n",
    "# 예시 입력\n",
    "src = torch.randint(0, input_dim, (batch_size, seq_len))  # (batch_size, seq_len) 원본 시퀀스\n",
    "tgt = torch.randint(0, vocab_size, (batch_size, seq_len))  # (batch_size, seq_len) 타겟 시퀀스\n",
    "\n",
    "# Transformer 모델 생성\n",
    "transformer = Transformer(num_layers=6, input_dim=input_dim, d_model=d_model, dff=dff, vocab_size=vocab_size, dropout=dropout)\n",
    "\n",
    "# Forward pass\n",
    "output = transformer(src, tgt)\n",
    "\n",
    "print(output.shape)  # (batch_size, seq_len, vocab_size)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "welcomedl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
